# Deep Learning Adversarial Attacks
[AI Classifiers Robustness to Adversarial Attacks:  Manipulating Output of Classifiers by Small Input  Perturbations.](http://dx.doi.org/10.13140/RG.2.2.33337.29286)

![](https://github.com/ghazigh/Deep-Learning-Adversarial-Attacks/blob/112a4b07fd1b0380fb9aad1176f4eb98147db27d/Adv%20Example.jpg)

This repo contains the code for my graduation project [Thesis](http://dx.doi.org/10.13140/RG.2.2.33337.29286) performed at the University of California Irvine under the supervision of Pr. Zhiying WANG.

Please cite this paper if you use our code or system output.
```
@phdthesis{phdthesis,
author = {Gharsallah, Ghazi},
year = {2020},
month = {06},
pages = {},
title = {AI Classifiers Robustness to Adversarial Attacks: Manipulating Output of Classifiers by Small Input Perturbations},
doi = {10.13140/RG.2.2.33337.29286}
}
```

Deep Learning has shown an advanced performance in the state of the art applications in several domains. However, its vulnerability to adversarial attacks can remarkably ruin any powerful classifier's performance through manipulating the output estimation with small perturbations, that provoked Machine Learning research community to go through this topic. Hence, the explanation of the reason of this vulnerability remains undecided.

In this project, we present the explanation of the existence of adversarial examples, and we analyze the features' behavior to adversarial perturbations, its responsibility toward this phenomena, and its impact on the robustness of an AI Classifier through the presentation of some experimental results using handwritten digits recognition using the MNIST dataset.

The project is motivated by two perspectives of the explanation of the existence of adversarial examples.

Finally, we present an evaluation of the proposed robustness enhancement method based on the explanation, that proves the efficiency of the proposed solution that could be applied on bigger scale applications to show more concrete performance.

# Result of classification of filters into Robust and Unrobust:
![](https://github.com/ghazigh/Deep-Learning-Adversarial-Attacks/blob/112a4b07fd1b0380fb9aad1176f4eb98147db27d/output_page-0001.jpg)

To understand the code , I have provided a [jupyter notebook](https://github.com/ghazigh/Deep-Learning-Adversarial-Attacks/blob/5e484369c50eb6cef8dba079edac21bcf3a121a8/Creation_And_Study_Of_Adversarial_Examples.ipynb)
